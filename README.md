# X Monitor

AI Agent for monitoring X.com (Twitter) accounts and generating daily summaries with LLM-powered analysis.

## Features

- ðŸ“± Monitor multiple X/Twitter accounts
- ðŸ¤– LLM-powered multi-dimensional analysis using OpenAI GPT
- ðŸ“Š Daily summaries with key insights extraction
- ðŸ”„ Incremental tweet fetching (only fetch new tweets since last run)
- ðŸ’¾ User info caching (reduce API calls per run)
- âš¡ Smart rate limiting with skip-on-limit strategy
- ðŸ“„ Auto-export Markdown reports to `output/` directory
- ðŸ” Regenerate reports from database (zero API calls)
- ðŸ“§ Email notifications (beautiful HTML formatted reports)
- ðŸ“² Telegram bot notifications (smart chunking, full content)
- â° Cron-based scheduled daily jobs
- ðŸ—„ï¸ Hybrid storage: JSON config + SQLite data persistence

## Installation

```bash
# Clone the repository
cd x-monitor

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -e .

# For development
pip install -e ".[dev]"
```

Requires Python 3.11+.

## Configuration

### 1. Environment variables

Copy and edit the example file:

```bash
cp .env.example .env
```

**Required:**

| Variable | Description |
|----------|-------------|
| `X_BEARER_TOKEN` | X API Bearer Token ([Developer Portal](https://developer.twitter.com/)) |
| `OPENAI_API_KEY` | OpenAI API Key ([Platform](https://platform.openai.com/)) |

**Optional â€” OpenAI:**

| Variable | Default | Description |
|----------|---------|-------------|
| `OPENAI_MODEL` | `gpt-4-turbo-preview` | Model to use for analysis |
| `OPENAI_MAX_COMPLETION_TOKENS` | `16000` | Max completion tokens |
| `OPENAI_TEMPERATURE` | *(model default)* | Temperature (leave empty for reasoning models) |

**Optional â€” Telegram notifications:**

| Variable | Description |
|----------|-------------|
| `TELEGRAM_BOT_TOKEN` | Bot token from [@BotFather](https://t.me/BotFather) |
| `TELEGRAM_CHAT_ID` | Target chat ID |

**Optional â€” Email notifications:**

| Variable | Default | Description |
|----------|---------|-------------|
| `SMTP_HOST` | `smtp.gmail.com` | SMTP server |
| `SMTP_PORT` | `587` | SMTP port (TLS) |
| `SMTP_USER` | | Sender email |
| `SMTP_PASSWORD` | | App-specific password |
| `EMAIL_TO` | | Recipient email |

**Optional â€” Scheduling & Rate limiting:**

| Variable | Default | Description |
|----------|---------|-------------|
| `SUMMARY_CRON_HOUR` | `8` | Daily job hour (0-23) |
| `SUMMARY_CRON_MINUTE` | `0` | Daily job minute (0-59) |
| `DATABASE_PATH` | `data/x_monitor.db` | SQLite database path |
| `RATE_LIMIT_DELAY` | `2.0` | Delay between accounts (seconds) |
| `RATE_LIMIT_BATCH_SIZE` | `10` | Accounts per batch |
| `RATE_LIMIT_BATCH_DELAY` | `10.0` | Delay between batches (seconds) |

### 2. Accounts to monitor

Edit `config/accounts.json` to add Twitter accounts:

```json
{
  "accounts": [
    {
      "username": "elonmusk",
      "note": "Tesla/SpaceX CEO"
    },
    {
      "username": "OpenAI",
      "note": "AI Research Company"
    }
  ]
}
```

When accounts are added via CLI (`x-monitor add`), `user_id`, `display_name`, and `description` are automatically fetched from the API and cached in this file, reducing API calls on subsequent runs.

## Usage

### List monitored accounts

```bash
x-monitor list
```

### Add/Remove accounts

```bash
# Add account (fetches and caches user info from API)
x-monitor add karpathy

# Remove account
x-monitor remove karpathy
```

### Run analysis immediately

```bash
x-monitor run
```

Fetches new tweets incrementally, analyzes with LLM, and sends notifications.

### Regenerate report from database

```bash
# Regenerate report for today (no API calls, uses cached tweets)
x-monitor regenerate

# Regenerate report for a specific date
x-monitor regenerate --date 2026-02-08

# Regenerate and send notifications
x-monitor regenerate --notify
```

This command reads tweets already stored in the local database and regenerates the LLM analysis **without making any X API calls**. 

**Use cases:**
- ðŸ§ª **Testing prompts** - Modified `src/analyzers/llm_analyzer.py`? Regenerate to see new analysis instantly
- ðŸ’° **Save API quota** - No X API or additional OpenAI calls (only LLM analysis)
- ðŸ“œ **Historical reports** - Generate reports for past dates from cached data
- ðŸ”§ **Fix errors** - If a report generation failed, rerun without re-fetching tweets

**What happens:**
1. Query all tweets from database for the specified date range
2. Send to LLM for fresh analysis using current prompts
3. Update database summary record
4. Generate/update Markdown report in `output/`
5. Optionally send notifications (with `--notify` flag)

### Start as a scheduled service

```bash
x-monitor serve
```

Runs the daily job at the configured time (default: 8:00 AM). Keeps running until Ctrl+C.

### View history

```bash
x-monitor history --days 7
```

## Output & Notifications

X Monitor generates reports in **three formats**, all sharing the same structure:

### 1. Markdown Files (Local)

**Location:** `output/report_YYYY-MM-DD.md`

- Auto-generated after each run
- Git-ignored (`.gitignore` configured)
- Full analysis content with formatting preserved
- Easy to read, search, and version control manually if needed

### 2. Email (HTML + Plain Text)

**Format:** Beautiful HTML email with modern styling

- **Metadata card** - Date, account count, tweet count, generation time
- **Full analysis** - All analysis dimensions (not truncated)
- **Key insights** - Highlighted in green cards
- **Responsive design** - Works across email clients
- **Plain text fallback** - For email clients that don't support HTML

**Sample structure:**
```
ðŸ“Š X/Twitter æ¯æ—¥ç›‘æŽ§æŠ¥å‘Š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ—¥æœŸï¼š2026å¹´02æœˆ09æ—¥           â”‚
â”‚ ç›‘æŽ§è´¦å·ï¼š14 ä¸ª                â”‚
â”‚ æŽ¨æ–‡æ•°é‡ï¼š147 æ¡               â”‚
â”‚ ç”Ÿæˆæ—¶é—´ï¼š2026-02-09 18:39:19 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[Full analysis content...]

å…³é”®æ´žå¯Ÿ
âœ“ Insight 1
âœ“ Insight 2
```

### 3. Telegram (Plain Text)

**Format:** Plain text with Unicode separators

- **Auto-chunking** - Messages >4096 chars split intelligently by line
- **Full content** - No truncation (previously limited to 3000 chars)
- **Reliable** - No Markdown parsing errors (removed complex escaping)
- **Numbered parts** - Multi-part messages labeled `(ç»­ 2/3)`

**Why plain text?** Telegram's MarkdownV2 has complex escaping rules that frequently caused parsing errors. Plain text is 100% reliable while maintaining readability.

### Notification Configuration

Configure in `.env`:

```bash
# Email
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=your_email@gmail.com
SMTP_PASSWORD=your_app_password
EMAIL_TO=recipient@example.com

# Telegram
TELEGRAM_BOT_TOKEN=your_bot_token
TELEGRAM_CHAT_ID=your_chat_id
```

Notifications are **optional** - leave variables unset to disable.

## Architecture

```
CLI (main.py)
    â”‚
    â–¼
Agent (agent.py) â”€â”€â”€ Main orchestrator
    â”‚
    â”œâ”€â”€ Storage (storage.py) â”€â”€â”€ Accounts (JSON) + Tweets/Summaries (SQLite)
    â”œâ”€â”€ Scraper (x_scraper.py) â”€â”€â”€ XDK API calls with rate limiting
    â”œâ”€â”€ Analyzer (llm_analyzer.py) â”€â”€â”€ OpenAI LLM analysis
    â””â”€â”€ Notifiers (email + telegram) â”€â”€â”€ Send formatted reports
```

**Daily job flow:**

1. Load accounts from `config/accounts.json`
2. Ensure all accounts have cached `user_id` (fetch from API if missing)
3. Build per-account "since" times from last saved tweet timestamps
4. Fetch only **new** tweets incrementally from X API
5. Save tweets to SQLite database
6. Load all tweets from last 24h from local database
7. Send to LLM for multi-dimensional analysis
8. Save summary to database
9. **Export Markdown report to `output/report_YYYY-MM-DD.md`**
10. Send notifications (Email + Telegram, if configured)

**Report formats:**
- ðŸ“„ **Markdown file** - Saved to `output/` directory (git-ignored)
- ðŸ“§ **Email (HTML)** - Modern styled HTML with full analysis content
- ðŸ“² **Telegram (Plain text)** - Auto-chunked for messages >4096 chars

All three formats share the same structure: metadata + full analysis + key insights.

## Project Structure

```
x-monitor/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrapers/       # X/Twitter data fetching (XDK)
â”‚   â”œâ”€â”€ analyzers/      # LLM multi-dimensional analysis
â”‚   â”œâ”€â”€ notifiers/      # Email & Telegram notifications
â”‚   â”œâ”€â”€ schedulers/     # Cron-based job scheduling
â”‚   â”œâ”€â”€ models/         # Pydantic data models
â”‚   â”œâ”€â”€ agent.py        # Main orchestrator
â”‚   â”œâ”€â”€ config.py       # Settings management (pydantic-settings)
â”‚   â”œâ”€â”€ storage.py      # Hybrid storage (JSON + SQLite)
â”‚   â””â”€â”€ main.py         # CLI entry point (Click)
â”œâ”€â”€ config/             # Account list (accounts.json)
â”œâ”€â”€ data/               # SQLite database (tweets, summaries)
â”œâ”€â”€ output/             # Generated Markdown reports (git-ignored)
â”œâ”€â”€ logs/               # Log files
â””â”€â”€ tests/              # Test files
```

## Tech Stack

- **X API**: [XDK](https://pypi.org/project/xdk/) (official SDK)
- **LLM**: OpenAI GPT
- **CLI**: Click
- **Data validation**: Pydantic
- **Storage**: aiosqlite + JSON
- **Notifications**: python-telegram-bot, aiosmtplib
- **Scheduling**: APScheduler
- **Logging**: Loguru

## License

MIT
