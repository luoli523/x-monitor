"""LLM-based tweet analyzer using OpenAI."""

from datetime import datetime
from loguru import logger
from openai import AsyncOpenAI

from src.models import Tweet, DailySummary


SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ AI é¢†åŸŸåˆ†æå¸ˆå’Œåˆ›ä¸šé¡¾é—®ï¼Œä¸“æ³¨äºä»¥ä¸‹é¢†åŸŸï¼š
- LLM/å¤§æ¨¡å‹æŠ€æœ¯å­¦ä¹ ä¸å®è·µ
- AI å·¥å…·å’ŒæŠ€æœ¯çš„æœ€æ–°æ¢ç´¢
- AI åˆ›ä¸šæœºä¼šå’Œå•†ä¸šæ¨¡å¼
- ç”¨ AI èµšé’±çš„å®æˆ˜ç»éªŒå’Œæ–¹æ³•

ä½ çš„åˆ†æé£æ ¼ï¼š
- åŠ¡å®ï¼šå…³æ³¨å¯è½åœ°ã€å¯æ‰§è¡Œçš„å†…å®¹
- æ•é”ï¼šæ•æ‰æ½œåœ¨çš„å•†ä¸šæœºä¼šå’ŒæŠ€æœ¯è¶‹åŠ¿
- ç²¾ç‚¼ï¼šæç‚¼æœ€æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼Œä¸è¯´åºŸè¯
- è¡ŒåŠ¨å¯¼å‘ï¼šç»™å‡ºå…·ä½“çš„å­¦ä¹ æˆ–è¡ŒåŠ¨å»ºè®®

è¯·ä½¿ç”¨ä¸­æ–‡å›å¤ã€‚"""


class LLMAnalyzer:
    """Analyzer using OpenAI for tweet analysis."""

    def __init__(
        self,
        api_key: str,
        model: str = "gpt-4-turbo-preview",
        max_completion_tokens: int = 16000,
        temperature: float | None = None,
    ):
        """Initialize the analyzer.

        Args:
            api_key: OpenAI API key
            model: Model name to use
            max_completion_tokens: Maximum tokens for completion
            temperature: Temperature for sampling (None = model default)
        """
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = model
        self.max_completion_tokens = max_completion_tokens
        self.temperature = temperature

    def _format_tweets_for_analysis(self, tweets: list[Tweet]) -> str:
        """Format tweets into a string for LLM analysis."""
        if not tweets:
            return "æ²¡æœ‰æ¨æ–‡æ•°æ®ã€‚"

        # Group by author
        by_author: dict[str, list[Tweet]] = {}
        for tweet in tweets:
            if tweet.author_username not in by_author:
                by_author[tweet.author_username] = []
            by_author[tweet.author_username].append(tweet)

        lines = []
        for author, author_tweets in by_author.items():
            display_name = author_tweets[0].author_display_name or author
            lines.append(f"\n## @{author} ({display_name})")
            lines.append(f"å…± {len(author_tweets)} æ¡æ¨æ–‡\n")

            for tweet in author_tweets[:10]:  # Limit per author
                time_str = tweet.created_at.strftime("%Y-%m-%d %H:%M")
                engagement = f"â¤ï¸{tweet.likes} ğŸ”{tweet.retweets} ğŸ’¬{tweet.replies}"

                prefix = ""
                if tweet.is_retweet:
                    prefix = "[è½¬æ¨] "
                elif tweet.is_reply:
                    prefix = "[å›å¤] "

                lines.append(f"- [{time_str}] {prefix}{tweet.content[:200]}")
                lines.append(f"  {engagement}")
                lines.append(f"  {tweet.url}\n")

        return "\n".join(lines)

    async def analyze_tweets(self, tweets: list[Tweet], date: datetime) -> DailySummary:
        """Analyze tweets and generate daily summary.

        Args:
            tweets: List of tweets to analyze
            date: The date being summarized

        Returns:
            DailySummary with LLM-generated content
        """
        date_str = date.strftime("%Yå¹´%mæœˆ%dæ—¥")
        formatted_tweets = self._format_tweets_for_analysis(tweets)

        # Get unique authors
        authors = set(t.author_username for t in tweets)

        user_prompt = f"""è¯·åˆ†æä»¥ä¸‹ {date_str} çš„æ¨æ–‡æ•°æ®ï¼Œä» AI å­¦ä¹ è€…å’Œåˆ›ä¸šè€…çš„è§†è§’æä¾›æ·±åº¦åˆ†æï¼š

## åˆ†æç»´åº¦

### 1. ğŸ”¥ ä»Šæ—¥å¿…çœ‹ï¼ˆæœ€é‡è¦ï¼Œæ”¾åœ¨æœ€å‰é¢ï¼‰
ä»æ‰€æœ‰æ¨æ–‡ä¸­ç²¾é€‰ 3-5 æ¡æœ€å€¼å¾—å…³æ³¨çš„å†…å®¹ï¼Œè¯´æ˜ï¼š
- ä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨
- åŸæ–‡é“¾æ¥
- å»ºè®®çš„è¡ŒåŠ¨ï¼ˆå­¦ä¹ /å®è·µ/æ”¶è—/æ·±å…¥ç ”ç©¶ï¼‰

### 2. ğŸ“š LLM å­¦ä¹ ä¸æŠ€æœ¯å®è·µ
- æœ‰å“ªäº›å…³äºå¤§æ¨¡å‹ã€Prompt å·¥ç¨‹ã€Agent å¼€å‘çš„å¹²è´§ï¼Ÿ
- æœ‰ä»€ä¹ˆæ–°å·¥å…·ã€æ–°æ¡†æ¶ã€æ–°æŠ€æœ¯å€¼å¾—å­¦ä¹ ï¼Ÿ
- æå–å¯ä»¥ç›´æ¥å­¦ä¹ æˆ–å¤ç°çš„å†…å®¹

### 3. ğŸ’¡ AI åˆ›ä¸šçµæ„Ÿ
- å‘ç°äº†å“ªäº› AI äº§å“åˆ›æ„æˆ–å•†ä¸šæœºä¼šï¼Ÿ
- æœ‰ä»€ä¹ˆå¯ä»¥å¿«é€ŸéªŒè¯çš„ MVP æƒ³æ³•ï¼Ÿ
- å•äºº/å°å›¢é˜Ÿå¯ä»¥åšçš„é¡¹ç›®æœ‰å“ªäº›ï¼Ÿ

### 4. ğŸ’° AI èµšé’±å®æˆ˜
- æœ‰å“ªäº›ç”¨ AI èµšé’±çš„çœŸå®æ¡ˆä¾‹æˆ–æ–¹æ³•ï¼Ÿ
- æœ‰ä»€ä¹ˆå¯å¤åˆ¶çš„å˜ç°æ¨¡å¼ï¼Ÿ
- æå–å…·ä½“çš„æ•°æ®å’Œæ”¶ç›Šæƒ…å†µï¼ˆå¦‚æœ‰ï¼‰

### 5. ğŸ¯ å…³é”®æ´å¯Ÿ
åˆ—å‡º 3-5 æ¡æœ€é‡è¦çš„å‘ç°ï¼Œæ¯æ¡åŒ…å«ï¼š
- æ´å¯Ÿå†…å®¹
- ä¿¡æ¯æ¥æºï¼ˆå“ªä¸ªè´¦å·ï¼‰
- ä¸ºä»€ä¹ˆé‡è¦

### 6. ğŸ“‹ è´¦å·åŠ¨æ€é€Ÿè§ˆ
ç®€è¦æ€»ç»“æ¯ä¸ªæ´»è·ƒè´¦å·ä»Šå¤©å‘äº†ä»€ä¹ˆï¼ˆ1-2å¥è¯/è´¦å·ï¼‰

---

æ¨æ–‡æ•°æ®ï¼š
{formatted_tweets}

è¯·ç¡®ä¿åˆ†æå…·æœ‰å¯æ“ä½œæ€§ï¼Œçªå‡º"ä»Šæ—¥å¿…çœ‹"éƒ¨åˆ†ã€‚"""

        try:
            # Build request kwargs
            request_kwargs = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                "max_completion_tokens": self.max_completion_tokens,
            }
            # Only add temperature if specified (some models don't support it)
            if self.temperature is not None:
                request_kwargs["temperature"] = self.temperature

            response = await self.client.chat.completions.create(**request_kwargs)

            analysis_text = response.choices[0].message.content or ""
            logger.info(f"Generated analysis for {len(tweets)} tweets")
            logger.debug(f"Raw response: {response}")
            logger.info(f"Analysis text length: {len(analysis_text)}")

            # Extract key insights (from "ä»Šæ—¥å¿…çœ‹" or "å…³é”®æ´å¯Ÿ" sections)
            key_insights = []
            lines = analysis_text.split("\n")
            in_section = False
            section_keywords = ["ä»Šæ—¥å¿…çœ‹", "å…³é”®æ´å¯Ÿ", "å…³é”®å‘ç°", "å¿…çœ‹"]

            for line in lines:
                # Check if entering a key section
                if any(kw in line for kw in section_keywords):
                    in_section = True
                    continue
                # Check if leaving the section (new header)
                if in_section and line.strip().startswith("##"):
                    if len(key_insights) >= 3:  # Got enough from this section
                        in_section = False
                        continue
                # Extract insights
                if in_section and line.strip().startswith(("-", "â€¢", "1", "2", "3", "4", "5", "*")):
                    insight = line.strip().lstrip("-â€¢*0123456789. ")
                    if insight and len(insight) > 10:  # Filter out too short lines
                        key_insights.append(insight)
                if len(key_insights) >= 5:
                    break

            return DailySummary(
                date=date,
                accounts_monitored=len(authors),
                total_tweets=len(tweets),
                tweets=tweets,
                summary_text=analysis_text,
                analysis=analysis_text,
                key_insights=key_insights[:5],
            )

        except Exception as e:
            logger.error(f"Error analyzing tweets: {e}")
            return DailySummary(
                date=date,
                accounts_monitored=len(authors),
                total_tweets=len(tweets),
                tweets=tweets,
                summary_text=f"åˆ†æç”Ÿæˆå¤±è´¥: {e}",
                analysis="",
            )
